{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Gutenberg.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.4 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "5eb756464d92b1fba7c316c219a9aedd64e50b1f9cf7b2745bd052621490d6a2"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import requests\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import re\r\n",
        "import pandas as pd\r\n",
        "import numpy as np"
      ],
      "outputs": [],
      "metadata": {
        "id": "SAp9XBnUjYi2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialise downloads page and get links for the first 4 pages"
      ],
      "metadata": {
        "id": "0rAppRLuUEzy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "url = '''https://www.gutenberg.org/ebooks/search/?sort-order=downloads&sort_order=downloads''' # Downloads page\r\n",
        "start_indices = [str(index+1) for index in range(0,76,25)]\r\n",
        "responses = [requests.get(url + '&start_index=' + index) for index in start_indices]\r\n",
        "\r\n",
        "books_soups = [BeautifulSoup(response.text, 'html.parser') for response in responses] # Get soups for each page"
      ],
      "outputs": [],
      "metadata": {
        "id": "bkgan3s-jqEM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get all \\<a\\> tags with class of link in each of the soups"
      ],
      "metadata": {
        "id": "SG57UgHRUekf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "# Get all the <a> tag with class of 'link'\r\n",
        "book_tags = []\r\n",
        "\r\n",
        "for books_soup in books_soups:\r\n",
        "  book_tags.extend(books_soup.find_all('a', attrs={'class': \"link\"}))\r\n",
        "\r\n",
        "# Get the name of the link to each book download page (testing if the href starts with '/ebooks' and ends with an integer)\r\n",
        "book_tags = [tag.attrs['href'] for tag in book_tags \r\n",
        "             if tag.attrs['href'].startswith('/ebooks') & \r\n",
        "            tag.attrs['href'].endswith(tuple([str(digit) for digit in range(0,10)]))]\r\n",
        "\r\n",
        "# Display book tags\r\n",
        "print(\"In total we have \" + str(len(book_tags)) + \" book titles\")\r\n",
        "print(\"Displaying 10 titles\")\r\n",
        "print(book_tags[:10]) "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In total we have 100 book titles\n",
            "Displaying 10 titles\n",
            "['/ebooks/1342', '/ebooks/84', '/ebooks/11', '/ebooks/1661', '/ebooks/2701', '/ebooks/1232', '/ebooks/174', '/ebooks/98', '/ebooks/16328', '/ebooks/64317']\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdOyWDSLoU09",
        "outputId": "1e583323-0c29-4a15-ec0f-27239eb5fd00"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "for book_soup in books_soups:\r\n",
        "  book_soup.find_all('')"
      ],
      "outputs": [],
      "metadata": {
        "id": "sGRZGs4K_kIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combine href attribute with Gutenberg URL to get link for each e-book"
      ],
      "metadata": {
        "id": "UB9bpgsuVJqt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "base_url = \"https://www.gutenberg.org\"\r\n",
        "\r\n",
        "# Get e-book links\r\n",
        "book_links = [base_url + tag for tag in book_tags]\r\n",
        "print(\"In total we have \" + str(len(book_links)) + \" books\")\r\n",
        "print(\"Displaying 10 book links\")\r\n",
        "print(book_links[:10])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In total we have 100 books\n",
            "Displaying 10 book links\n",
            "['https://www.gutenberg.org/ebooks/1342', 'https://www.gutenberg.org/ebooks/84', 'https://www.gutenberg.org/ebooks/11', 'https://www.gutenberg.org/ebooks/1661', 'https://www.gutenberg.org/ebooks/2701', 'https://www.gutenberg.org/ebooks/1232', 'https://www.gutenberg.org/ebooks/174', 'https://www.gutenberg.org/ebooks/98', 'https://www.gutenberg.org/ebooks/16328', 'https://www.gutenberg.org/ebooks/64317']\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TtjSpoYpAVY",
        "outputId": "87411272-4d55-423c-c73c-35aad6b1bfb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the book names and author names"
      ],
      "metadata": {
        "id": "9PKM1_xFV0Il"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "book_names = []\r\n",
        "book_authors = []\r\n",
        "for link in book_links:\r\n",
        "  book_soup = BeautifulSoup(requests.get(link).text, 'html.parser') # Get the soup of each book link\r\n",
        "  # Get the header of each book link which contains the name of book and the author separated by the word 'by'\r\n",
        "  book_and_author = book_soup.find('h1').get_text().split(\" by \")\r\n",
        "  # If no author displayed add an empty string\r\n",
        "  if len(book_and_author) == 1:\r\n",
        "    book_and_author.append('')\r\n",
        "\r\n",
        "  # Add book names and authors to list\r\n",
        "  book_names.append(book_and_author[0])\r\n",
        "  book_authors.append(book_and_author[1])"
      ],
      "outputs": [],
      "metadata": {
        "id": "vHJqZvm1YoX_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the html or pdf links to all the books"
      ],
      "metadata": {
        "id": "69WfQJzkWSh3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "book_links_soups = [BeautifulSoup(requests.get(book_link).text, 'html.parser') for book_link in book_links]\r\n",
        "\r\n",
        "full_book_links = []\r\n",
        "for soup in book_links_soups:\r\n",
        "  # Find the html copy of the book in each page by searching for the correct link\r\n",
        "  book_tag = soup.find('a', attrs={'type': re.compile(r'^text/html')})\r\n",
        "  # If no html copy exists, get the pdf version\r\n",
        "  if book_tag is None:\r\n",
        "    book_tag = soup.find('a', attrs={'type': \"application/pdf\"})\r\n",
        "  # Add to list\r\n",
        "  full_book_links.append(book_tag)\r\n",
        "\r\n",
        "full_book_links = [base_url + link['href'] for link in full_book_links]"
      ],
      "outputs": [],
      "metadata": {
        "id": "JmZosoFrSZd0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "len(full_book_links)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM1ZMnparP-F",
        "outputId": "17d97058-c110-4d3d-8398-74c8738de620"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define function to obtain and clean text from each book"
      ],
      "metadata": {
        "id": "wBgOMjbrDliD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "source": [
        "from random import randint\r\n",
        "                \r\n",
        "# List of words to skip over when extracting paragraphs\r\n",
        "exclude_words = ['gutenberg', 'transcribe', 'ebook', '[', ']', 'publish',\r\n",
        "                 'manuscript', 'text', 'writer', 'content', 'author', 'title', \r\n",
        "                 'illustrat', 'chapter', 'edit', 'reserve', 'copyright', \r\n",
        "                 'rights', 'epilogue', 'part', 'canto']\r\n",
        "\r\n",
        "def get_excerpts(book_links):\r\n",
        "  '''Extracts text excerpts from a list of book links and cleans the text'''\r\n",
        "\r\n",
        "  # Initialise excerpts list\r\n",
        "  excerpts = []\r\n",
        "\r\n",
        "  for i in range(0, len(book_links)):\r\n",
        "    text = []\r\n",
        "\r\n",
        "    # Get book soups, ignoring 'pdf's and attempt to decode as utf-8\r\n",
        "    if book_links[i][-3:] == \"pdf\":\r\n",
        "      excerpts.append(' ')\r\n",
        "      continue\r\n",
        "    try:\r\n",
        "      book_soup = BeautifulSoup(requests.get(book_links[i]).content.decode(\"utf-8\"), 'html.parser')\r\n",
        "    except UnicodeDecodeError:\r\n",
        "      book_soup = BeautifulSoup(requests.get(book_links[i]).content, 'html.parser')\r\n",
        "\r\n",
        "    # Exception for Dante's Inferno: link provides a contents page linking to other webpages which contain the text\r\n",
        "    if book_links[i].endswith('8800.html.images'):\r\n",
        "      child_link = book_soup.find('a', attrs = {'href': re.compile(r'link[0-9]+$')})['href']\r\n",
        "\r\n",
        "      child_soup = BeautifulSoup(requests.get(child_link).content.decode(\"utf-8\"), 'html.parser')\r\n",
        "\r\n",
        "      # Find all paragraph tags\r\n",
        "      paragraph_tags = child_soup.find_all('p')\r\n",
        "    else:\r\n",
        "      paragraph_tags = book_soup.find_all('p')\r\n",
        "\r\n",
        "    # Skip random number of tags to get paragraph deeper in text\r\n",
        "    skip_num = randint(10, len(paragraph_tags) - 15)\r\n",
        "\r\n",
        "    # Iterate through each paragraph tag\r\n",
        "    for num, tag in enumerate(paragraph_tags):\r\n",
        "\r\n",
        "      # Remove paragraph tags which display the book chapters\r\n",
        "      try:\r\n",
        "        if 'toc' in tag.attrs['class']:\r\n",
        "          continue\r\n",
        "      except:\r\n",
        "        pass\r\n",
        "      \r\n",
        "      # Skip a certain number of paragraphs\r\n",
        "      if num <= skip_num:\r\n",
        "        continue\r\n",
        "\r\n",
        "      # Obtain paragraph tag text\r\n",
        "      paragraph_text = tag.get_text()\r\n",
        "\r\n",
        "      # Remove new lines and join them with spaces\r\n",
        "      try: \r\n",
        "        paragraph_text = ' '.join(paragraph_text.split())\r\n",
        "      except AttributeError:\r\n",
        "        continue\r\n",
        "\r\n",
        "      # Exclude paragraph tags with certain key words\r\n",
        "      if any(substring in paragraph_text.lower() for substring in exclude_words):\r\n",
        "        continue\r\n",
        "    \r\n",
        "      # Add the text in the current <p> tag to the list\r\n",
        "      text.append(paragraph_text)\r\n",
        "\r\n",
        "      # Join sections from each <p> tag in the list\r\n",
        "      text = [' '.join(text)]\r\n",
        "\r\n",
        "      # Remove garbage text from Dante's Inferno\r\n",
        "      if book_links[i].endswith(\"8800.html.images\"):\r\n",
        "        text[0] = text[0].replace('ENLARGE TO FULL SIZE', '')\r\n",
        "\r\n",
        "      # Cut if the text length exceeds 100\r\n",
        "      if len(text[0].split()) >= 100 or num == len(paragraph_tags) - 1:\r\n",
        "\r\n",
        "        # Remove all caps words (some texts are plays and contain character names\r\n",
        "        # in all caps), but make exception for variants of I, A and O (rare but \r\n",
        "        # does exist)\r\n",
        "        temp = text[0].split()\r\n",
        "        I_or_A = re.compile(r'(^[—\"\\'“][AI])+|^[AI]+|^[—\"\\'“][O]')\r\n",
        "        text[0] = \" \".join([word for word in temp if not word.isupper() or re.match(I_or_A, word)])\r\n",
        "\r\n",
        "        split_text = text[0].split()\r\n",
        "        for j in range(len(split_text)):\r\n",
        "          # Checks if word ends with full stop to get full sentence, and if first \r\n",
        "          # letter is capital (avoid Mrs./Mr. etc...)\r\n",
        "          if split_text[j][-1] == \".\" and not split_text[j][0].isupper() and j >= 100:\r\n",
        "            text[0] = ' '.join(split_text[:j+1])\r\n",
        "            break\r\n",
        "\r\n",
        "        excerpts.append(text[0])\r\n",
        "        break\r\n",
        "      \r\n",
        "  return excerpts"
      ],
      "outputs": [],
      "metadata": {
        "id": "PefOA-CIQl5L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract excerpts multiple times and convert them to csv"
      ],
      "metadata": {
        "id": "DaseMI8ihCJX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for i in range(1,11):\r\n",
        "  print(\"***Batch \" + str(i) + \"***\")\r\n",
        "\r\n",
        "  # Get excerpts from book links\r\n",
        "  excerpts = get_excerpts(full_book_links)\r\n",
        "\r\n",
        "  # Add text and book identifiers to Pandas DataFrame\r\n",
        "  output = pd.DataFrame({'book': book_names, 'author': book_authors, 'url': full_book_links, 'excerpt': excerpts})\r\n",
        "\r\n",
        "  # Convert to csv\r\n",
        "  output.to_csv('gutenberg-excerpts-' + str(i) + '.csv', index = False, encoding=\"utf-8-sig\")"
      ],
      "outputs": [],
      "metadata": {}
    }
  ]
}
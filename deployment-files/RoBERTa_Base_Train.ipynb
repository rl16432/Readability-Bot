{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":["### Install Huggingface datasets library\r\n","\r\n","* Datasets library is required to package training data, both inputs and targets into a dataset compatible with the Huggingface Trainer API "],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["!pip install datasets"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-17T03:34:32.758814Z","iopub.execute_input":"2021-09-17T03:34:32.759440Z","iopub.status.idle":"2021-09-17T03:34:40.947222Z","shell.execute_reply.started":"2021-09-17T03:34:32.759346Z","shell.execute_reply":"2021-09-17T03:34:40.946242Z"},"trusted":true}},{"cell_type":"markdown","source":["### Import libraries"],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["# This Python 3 environment comes with many helpful analytics libraries installed\r\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\r\n","# For example, here's several helpful packages to load\r\n","\r\n","import numpy as np # linear algebra\r\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r\n","import random\r\n","\r\n","import torch\r\n","\r\n","from datasets.arrow_dataset import Dataset\r\n","\r\n","from sklearn.model_selection import train_test_split\r\n","\r\n","from transformers import AutoTokenizer\r\n","from transformers import Trainer\r\n","from transformers import TrainingArguments\r\n","from transformers import AutoModelForSequenceClassification\r\n","\r\n","# Input data files are available in the read-only \"../input/\" directory\r\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\r\n","\r\n","import os\r\n","for dirname, _, filenames in os.walk('/kaggle/input'):\r\n","    for filename in filenames:\r\n","        print(os.path.join(dirname, filename))\r\n","\r\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \r\n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"],"outputs":[{"output_type":"stream","name":"stderr","text":["2021-09-17 03:34:47.796777: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"]},{"output_type":"stream","name":"stdout","text":["/kaggle/input/commonlitreadabilityprize/sample_submission.csv\n","/kaggle/input/commonlitreadabilityprize/train.csv\n","/kaggle/input/commonlitreadabilityprize/test.csv\n"]}],"metadata":{"execution":{"iopub.status.busy":"2021-09-17T03:34:40.950416Z","iopub.execute_input":"2021-09-17T03:34:40.950666Z","iopub.status.idle":"2021-09-17T03:34:52.003149Z","shell.execute_reply.started":"2021-09-17T03:34:40.950639Z","shell.execute_reply":"2021-09-17T03:34:52.002377Z"},"trusted":true}},{"cell_type":"markdown","source":["### Seed everything for reproducibility\n","\n","* Considerable amount of randomness during model training would lead to different results each time the notebook is executed\n","\n","* Sources of randomness include dropout and weight initialisation"],"metadata":{}},{"cell_type":"code","execution_count":3,"source":["seed = 3\r\n","\r\n","# python RNG\r\n","random.seed(seed)\r\n","\r\n","# pytorch RNGs\r\n","torch.manual_seed(seed)\r\n","torch.backends.cudnn.deterministic = True\r\n","if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\r\n","\r\n","# numpy RNG\r\n","np.random.seed(seed)"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-17T03:34:52.004311Z","iopub.execute_input":"2021-09-17T03:34:52.005047Z","iopub.status.idle":"2021-09-17T03:34:52.062027Z","shell.execute_reply.started":"2021-09-17T03:34:52.005009Z","shell.execute_reply":"2021-09-17T03:34:52.061260Z"},"trusted":true}},{"cell_type":"markdown","source":["## Data preparation"],"metadata":{}},{"cell_type":"markdown","source":["### Load training and prediction data\n","\n","* Utilise pandas library to read in csv files for the training and test sets\n","* Columns are dropped in the train DataFrame to contain only useful data "],"metadata":{}},{"cell_type":"code","execution_count":4,"source":["source_dir = '/kaggle/input/commonlitreadabilityprize/'\r\n","train_data = pd.read_csv(source_dir + 'train.csv')\r\n","\r\n","# Drop unnecessary columns and rename target column to labels (required for Trainer)\r\n","train_data = train_data.drop(columns = [\"id\", \"url_legal\", \"license\", \"standard_error\"]).rename(columns = {\"target\": \"labels\"})\r\n","\r\n","preds_data = pd.read_csv(source_dir + 'test.csv')\r\n","\r\n","train_data.head()"],"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>excerpt</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>When the young people returned to the ballroom...</td>\n","      <td>-0.340259</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n","      <td>-0.315372</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>As Roger had predicted, the snow departed as q...</td>\n","      <td>-0.580118</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>And outside before the palace a great garden w...</td>\n","      <td>-1.054013</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Once upon a time there were Three Bears who li...</td>\n","      <td>0.247197</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                             excerpt    labels\n","0  When the young people returned to the ballroom... -0.340259\n","1  All through dinner time, Mrs. Fayre was somewh... -0.315372\n","2  As Roger had predicted, the snow departed as q... -0.580118\n","3  And outside before the palace a great garden w... -1.054013\n","4  Once upon a time there were Three Bears who li...  0.247197"]},"metadata":{},"execution_count":4}],"metadata":{"execution":{"iopub.status.busy":"2021-09-17T03:34:52.064094Z","iopub.execute_input":"2021-09-17T03:34:52.064348Z","iopub.status.idle":"2021-09-17T03:34:52.176743Z","shell.execute_reply.started":"2021-09-17T03:34:52.064317Z","shell.execute_reply":"2021-09-17T03:34:52.176020Z"},"trusted":true}},{"cell_type":"markdown","source":["### Replace new line indicators with spaces to remove unnecessary tokens"],"metadata":{}},{"cell_type":"code","execution_count":5,"source":["train_data['excerpt'] = train_data['excerpt'].apply(lambda x : x.replace('\\n', ' '))\r\n","preds_data['excerpt'] = preds_data['excerpt'].apply(lambda x : x.replace('\\n', ' '))\r\n","\r\n","train_data['excerpt'][0]"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["'When the young people returned to the ballroom, it presented a decidedly changed appearance. Instead of an interior scene, it was a winter landscape. The floor was covered with snow-white canvas, not laid on smoothly, but rumpled over bumps and hillocks, like a real snow field. The numerous palms and evergreens that had decorated the room, were powdered with flour and strewn with tufts of cotton, like snow. Also diamond dust had been lightly sprinkled on them, and glittering crystal icicles hung from the branches. At each end of the room, on the wall, hung a beautiful bear-skin rug. These rugs were for prizes, one for the girls and one for the boys. And this was the game. The girls were gathered at one end of the room and the boys at the other, and one end was called the North Pole, and the other the South Pole. Each player was given a small flag which they were to plant on reaching the Pole. This would have been an easy matter, but each traveller was obliged to wear snowshoes.'"]},"metadata":{},"execution_count":5}],"metadata":{"execution":{"iopub.status.busy":"2021-09-17T03:34:52.178206Z","iopub.execute_input":"2021-09-17T03:34:52.178468Z","iopub.status.idle":"2021-09-17T03:34:52.191380Z","shell.execute_reply.started":"2021-09-17T03:34:52.178434Z","shell.execute_reply":"2021-09-17T03:34:52.190352Z"},"trusted":true}},{"cell_type":"markdown","source":["### Split train.csv into training and validation sets to evaluate model training\n","\n","**Using scikit-learn's train_test_split:**\n","<br>\n","* Specify evaluation size to be 20% of the entire train set\n","* random_state argument shuffles the data and makes the shuffling reproducible"],"metadata":{}},{"cell_type":"code","execution_count":6,"source":["train_set, eval_set = train_test_split(train_data, test_size = 0.2, random_state = 42)\r\n","\r\n","train_set.head()"],"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>excerpt</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2743</th>\n","      <td>The building of rotary presses for printing il...</td>\n","      <td>-1.518350</td>\n","    </tr>\n","    <tr>\n","      <th>2347</th>\n","      <td>The idea of a trip on Bob's yacht suited every...</td>\n","      <td>-0.548807</td>\n","    </tr>\n","    <tr>\n","      <th>2387</th>\n","      <td>Seeing the front door wide open, the enchanter...</td>\n","      <td>-0.193262</td>\n","    </tr>\n","    <tr>\n","      <th>2202</th>\n","      <td>The widow she cried over me, and called me a p...</td>\n","      <td>-1.033799</td>\n","    </tr>\n","    <tr>\n","      <th>786</th>\n","      <td>Jacobitism was (and, to a much smaller extent,...</td>\n","      <td>-1.725606</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                excerpt    labels\n","2743  The building of rotary presses for printing il... -1.518350\n","2347  The idea of a trip on Bob's yacht suited every... -0.548807\n","2387  Seeing the front door wide open, the enchanter... -0.193262\n","2202  The widow she cried over me, and called me a p... -1.033799\n","786   Jacobitism was (and, to a much smaller extent,... -1.725606"]},"metadata":{},"execution_count":6}],"metadata":{"execution":{"iopub.status.busy":"2021-09-17T03:34:52.193253Z","iopub.execute_input":"2021-09-17T03:34:52.193606Z","iopub.status.idle":"2021-09-17T03:34:52.212120Z","shell.execute_reply.started":"2021-09-17T03:34:52.193568Z","shell.execute_reply":"2021-09-17T03:34:52.211267Z"},"trusted":true}},{"cell_type":"markdown","source":["### Convert pandas DataFrame to Dataset object\n","\n","* This is done in order to convert DataFrame into a type which is compatible with the in-built Huggingface Trainer"],"metadata":{}},{"cell_type":"code","execution_count":7,"source":["train_dataset = Dataset.from_pandas(df = train_set)\r\n","eval_dataset = Dataset.from_pandas(df = eval_set)\r\n","\r\n","train_dataset"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['excerpt', 'labels', '__index_level_0__'],\n","    num_rows: 2267\n","})"]},"metadata":{},"execution_count":7}],"metadata":{"execution":{"iopub.status.busy":"2021-09-17T03:34:52.213687Z","iopub.execute_input":"2021-09-17T03:34:52.214004Z","iopub.status.idle":"2021-09-17T03:34:52.245409Z","shell.execute_reply.started":"2021-09-17T03:34:52.213965Z","shell.execute_reply":"2021-09-17T03:34:52.244778Z"},"trusted":true}},{"cell_type":"markdown","source":["### Load pretrained RoBERTa tokenizer\n","\n","* Tokenizer has a pretrained set of vocabulary and assigns an integer to each word in a passage\n","* Convert text into numerical form to pass into the transformer"],"metadata":{}},{"cell_type":"code","execution_count":8,"source":["tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\r\n","tokenizer.save_pretrained('./Commonlit-RoBERTa-Base/tokenizer')\r\n","\r\n","tokenizer"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b54c0630703b4abf86ad88120dd06b1c"},"text/plain":["Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"069e7e871bd844f5a75c917a1d97f3ca"},"text/plain":["Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f43259e546a46efb53e3bc1d2a61ee7"},"text/plain":["Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2f4afe3dbe144ec88767dbbdfcc36d3"},"text/plain":["Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["PreTrainedTokenizerFast(name_or_path='roberta-base', vocab_size=50265, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)})"]},"metadata":{},"execution_count":8}],"metadata":{"execution":{"iopub.status.busy":"2021-09-17T03:34:52.246530Z","iopub.execute_input":"2021-09-17T03:34:52.246775Z","iopub.status.idle":"2021-09-17T03:35:00.485270Z","shell.execute_reply.started":"2021-09-17T03:34:52.246744Z","shell.execute_reply":"2021-09-17T03:35:00.484566Z"},"trusted":true}},{"cell_type":"markdown","source":["### Map tokenized text excerpts to Dataset\n","\n","**1.** Define a function which returns sequences of tokens, taking the texts as input in order to prepare the data for the model\n","    * The sequences are padded to the maximum length defined by the max_length argument\n","    * Truncation of the sequence occurs when the excerpt is too long\n","    * Sequences will have an attention mask with them, which 'masks' padded tokens and tells the model to disregard them\n","    * Token '1' correlates to the padding tokens<br><br>\n","**2.** Utilise map method on the Datasets to execute the 'tokenize_data' function on each excerpt stored in the Dataset<br><br>\n","**3.** Convert data to PyTorch tensors\n","\n","Reference: https://huggingface.co/transformers/training.html"],"metadata":{}},{"cell_type":"code","execution_count":14,"source":["MAX_LENGTH = 256\r\n","\r\n","def tokenize_data(dataset):\r\n","    token_sequence = tokenizer(dataset[\"excerpt\"], padding = \"max_length\", truncation = True, max_length = MAX_LENGTH)\r\n","    return token_sequence\r\n","\r\n","# Map tokenized text (input_ids, attention_mask) to new dataset\r\n","tokenized_train_dataset = train_dataset.map(tokenize_data, batched = True).remove_columns([\"__index_level_0__\", \"excerpt\"])\r\n","tokenized_eval_dataset = eval_dataset.map(tokenize_data, batched = True).remove_columns([\"__index_level_0__\", \"excerpt\"])\r\n","\r\n","# Convert into PyTorch tensors\r\n","tokenized_train_dataset.set_format(\"torch\")\r\n","tokenized_eval_dataset.set_format(\"torch\")\r\n","\r\n","tokenized_train_dataset['input_ids'][0]"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a35d3881d95441268214f2a2977c5845"},"text/plain":["  0%|          | 0/3 [00:00<?, ?ba/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b95aad0ba654feeb4bc6bf1fd07dd81"},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["tensor([    0,   133,   745,     9, 13351,  1766, 34578,    13, 12168, 22827,\n","         6665,    21,  3751,    25,   419,    25,   504,  5243,    50,   504,\n","         2545,    11,   928,     6,    30,     5,  1513,     6,    53,  4100,\n","          396,  1282,     6,    25,   117,   285,  4521,    34,   655,    57,\n","          156,     9,   143,  9879,   898,     4,    20, 32130,   368,     9,\n","            5,   928, 25596,   491,  4756,   357,   775,     4,    96,   504,\n","         4718,    41, 22827, 20879,  2225,     6,    41,    66, 14596,     9,\n","           39,   372,  8812,     6,    21, 11118,  2115,    10, 13351,  1766,\n","         1228,    61,    21,     6,   309,     7,    39,   445,     6, 11236,\n","           30,    10, 20399,   179,   661,  1440,  2367,  1054,     4,    20,\n","           78,    65,     6,   959,     6,   222,    45,    23,    70,   972,\n","            5,   723,  4501,     9, 22827,   675,  3569, 12168,     6,     8,\n","            6,   150,   277,  3563, 11236,    15,     5,   276,  9322,    21,\n","         2343,    11,     5,  2201,  3015, 14413,     9,   504,  5479,     6,\n","           63,   173,    21,  5063,    11,  1318,  3486, 16363,  9077,     7,\n","            5,   782,     9,    10,  2743, 18296, 22827,  2225,     4,    83,\n","          200,  3563,     6,    67,    15,  6318,    23,     5,   276,    86,\n","            6,  1887,     8,  1490,    30,     5,  5417,  1515, 20399,   179,\n","          661,     6,   221,     4,   726,  1180,   329,   594,     6,   115,\n","           45,    28,    26,     7,    33, 32069,     5,  7626,     4,  3139,\n","         1663,    21, 15050,   251,    71,     5,  1273,     9,     5,  3015,\n","        14413,     6,     8,   350,   628,     7,  6136,     5,  2408,   219,\n","          864,     4,     2,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1,     1,     1])"]},"metadata":{},"execution_count":14}],"metadata":{"execution":{"iopub.status.busy":"2021-09-17T03:39:00.375839Z","iopub.execute_input":"2021-09-17T03:39:00.376569Z","iopub.status.idle":"2021-09-17T03:39:02.346741Z","shell.execute_reply.started":"2021-09-17T03:39:00.376530Z","shell.execute_reply":"2021-09-17T03:39:02.345935Z"},"trusted":true}},{"cell_type":"code","execution_count":15,"source":["tokenized_train_dataset"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['attention_mask', 'input_ids', 'labels'],\n","    num_rows: 2267\n","})"]},"metadata":{},"execution_count":15}],"metadata":{"execution":{"iopub.status.busy":"2021-09-17T04:06:08.151678Z","iopub.execute_input":"2021-09-17T04:06:08.151973Z","iopub.status.idle":"2021-09-17T04:06:08.157522Z","shell.execute_reply.started":"2021-09-17T04:06:08.151923Z","shell.execute_reply":"2021-09-17T04:06:08.156652Z"},"trusted":true}},{"cell_type":"markdown","source":["## Model selection"],"metadata":{}},{"cell_type":"markdown","source":["### Load RoBERTa-Base model\n","\n","* RoBERTa transformer was pretrained on larger dataset, and generally performs better than BERT\n","* Attention mechanism is one factor which makes transformers more effective compared to RNN or LSTM, as it enables the model to model long-term dependencies more effectively and weight each word in terms of its significance\n","* Set **num_labels** to 1 to indicate a regression problem is involved"],"metadata":{}},{"cell_type":"code","execution_count":16,"source":["# Load model from Huggingface\r\n","model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels = 1)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2696bc55575a4899a090a8432d07ec30"},"text/plain":["Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"metadata":{"execution":{"iopub.status.busy":"2021-09-17T04:26:25.438993Z","iopub.execute_input":"2021-09-17T04:26:25.439659Z","iopub.status.idle":"2021-09-17T04:26:45.814869Z","shell.execute_reply.started":"2021-09-17T04:26:25.439623Z","shell.execute_reply":"2021-09-17T04:26:45.814216Z"},"trusted":true}},{"cell_type":"markdown","source":["### Send the model to GPU memory\n","\n","* Required to increase training rate drastically"],"metadata":{}},{"cell_type":"code","execution_count":17,"source":["%%capture\r\n","\r\n","# Send model to GPU\r\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n","model.to(device)"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-17T04:26:45.816434Z","iopub.execute_input":"2021-09-17T04:26:45.816765Z","iopub.status.idle":"2021-09-17T04:26:52.928569Z","shell.execute_reply.started":"2021-09-17T04:26:45.816728Z","shell.execute_reply":"2021-09-17T04:26:52.927755Z"},"trusted":true}},{"cell_type":"markdown","source":["### Set up Trainer instance to train model\n","\n","* Trained on 5 epochs and save checkpoints of the model at the end of each epoch\n","* Best model is loaded at the end and determined by the evaluation loss"],"metadata":{}},{"cell_type":"code","execution_count":18,"source":["batch_size = 16\r\n","\r\n","training_args = TrainingArguments(\r\n","    output_dir=\"./Commonlit-RoBERTa-Base-CP\", # Select model path for checkpoint\r\n","    overwrite_output_dir=True,\r\n","    num_train_epochs=5,\r\n","    per_device_train_batch_size = batch_size,\r\n","    per_device_eval_batch_size = batch_size,\r\n","    evaluation_strategy = 'epoch',\r\n","    save_strategy = \"epoch\", # Save checkpoint at end of each epoch\r\n","    metric_for_best_model = 'eval_loss',\r\n","    greater_is_better = False,\r\n","    load_best_model_at_end = True,\r\n","    report_to = \"none\",\r\n","    seed = 3)\r\n","\r\n","# Create Trainer\r\n","trainer = Trainer(\r\n","    model = model,\r\n","    args = training_args,\r\n","    train_dataset = tokenized_train_dataset,\r\n","    eval_dataset = tokenized_eval_dataset)"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-17T04:26:52.930022Z","iopub.execute_input":"2021-09-17T04:26:52.930281Z","iopub.status.idle":"2021-09-17T04:26:52.947836Z","shell.execute_reply.started":"2021-09-17T04:26:52.930245Z","shell.execute_reply":"2021-09-17T04:26:52.947096Z"},"trusted":true}},{"cell_type":"code","execution_count":19,"source":["# Train model with given parameters\r\n","trainer.train()"],"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running training *****\n","  Num examples = 2267\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 710\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='710' max='710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [710/710 05:54, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.408047</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>0.544274</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>0.444234</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.296400</td>\n","      <td>0.277034</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.296400</td>\n","      <td>0.333592</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 567\n","  Batch size = 16\n","Saving model checkpoint to ./Commonlit-RoBERTa-Base-CP/checkpoint-142\n","Configuration saved in ./Commonlit-RoBERTa-Base-CP/checkpoint-142/config.json\n","Model weights saved in ./Commonlit-RoBERTa-Base-CP/checkpoint-142/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 567\n","  Batch size = 16\n","Saving model checkpoint to ./Commonlit-RoBERTa-Base-CP/checkpoint-284\n","Configuration saved in ./Commonlit-RoBERTa-Base-CP/checkpoint-284/config.json\n","Model weights saved in ./Commonlit-RoBERTa-Base-CP/checkpoint-284/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 567\n","  Batch size = 16\n","Saving model checkpoint to ./Commonlit-RoBERTa-Base-CP/checkpoint-426\n","Configuration saved in ./Commonlit-RoBERTa-Base-CP/checkpoint-426/config.json\n","Model weights saved in ./Commonlit-RoBERTa-Base-CP/checkpoint-426/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 567\n","  Batch size = 16\n","Saving model checkpoint to ./Commonlit-RoBERTa-Base-CP/checkpoint-568\n","Configuration saved in ./Commonlit-RoBERTa-Base-CP/checkpoint-568/config.json\n","Model weights saved in ./Commonlit-RoBERTa-Base-CP/checkpoint-568/pytorch_model.bin\n","***** Running Evaluation *****\n","  Num examples = 567\n","  Batch size = 16\n","Saving model checkpoint to ./Commonlit-RoBERTa-Base-CP/checkpoint-710\n","Configuration saved in ./Commonlit-RoBERTa-Base-CP/checkpoint-710/config.json\n","Model weights saved in ./Commonlit-RoBERTa-Base-CP/checkpoint-710/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ./Commonlit-RoBERTa-Base-CP/checkpoint-568 (score: 0.2770340144634247).\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=710, training_loss=0.22970777431004483, metrics={'train_runtime': 354.8342, 'train_samples_per_second': 31.944, 'train_steps_per_second': 2.001, 'total_flos': 1491168517532160.0, 'train_loss': 0.22970777431004483, 'epoch': 5.0})"]},"metadata":{},"execution_count":19}],"metadata":{"execution":{"iopub.status.busy":"2021-09-17T04:26:52.949546Z","iopub.execute_input":"2021-09-17T04:26:52.949798Z","iopub.status.idle":"2021-09-17T04:32:47.839808Z","shell.execute_reply.started":"2021-09-17T04:26:52.949760Z","shell.execute_reply":"2021-09-17T04:32:47.839053Z"},"trusted":true}},{"cell_type":"markdown","source":["### Convert model to evaluation mode\n","\n","* Required to perform predictions, otherwise the model would still behave as if it was being trained and implement dropout, leading to inconsistent predictions"],"metadata":{}},{"cell_type":"code","execution_count":20,"source":["%%capture\r\n","# Convert model to evaluation mode to indicate model is ready for prediction\r\n","model.eval()"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-17T04:33:07.298115Z","iopub.execute_input":"2021-09-17T04:33:07.298720Z","iopub.status.idle":"2021-09-17T04:33:07.309949Z","shell.execute_reply.started":"2021-09-17T04:33:07.298682Z","shell.execute_reply":"2021-09-17T04:33:07.308400Z"},"trusted":true}},{"cell_type":"markdown","source":["### Save pre-trained model"],"metadata":{}},{"cell_type":"code","execution_count":21,"source":["trainer.save_model('./Commonlit-RoBERTa-Base')"],"outputs":[{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./Commonlit-RoBERTa-Base\n","Configuration saved in ./Commonlit-RoBERTa-Base/config.json\n","Model weights saved in ./Commonlit-RoBERTa-Base/pytorch_model.bin\n"]}],"metadata":{"execution":{"iopub.status.busy":"2021-09-17T04:33:09.658578Z","iopub.execute_input":"2021-09-17T04:33:09.658842Z","iopub.status.idle":"2021-09-17T04:33:10.882187Z","shell.execute_reply.started":"2021-09-17T04:33:09.658813Z","shell.execute_reply":"2021-09-17T04:33:10.881476Z"},"trusted":true}},{"cell_type":"markdown","source":["## Evaluation"],"metadata":{}},{"cell_type":"markdown","source":["### Make predictions\n","\n","* Tokenize data in the test dataset and pass it through model to obtain the scores\n","* Add scores to list and export the scores as csv file"],"metadata":{}},{"cell_type":"code","execution_count":22,"source":["preds_targets = []\r\n","\r\n","for excerpt in preds_data['excerpt']:\r\n","    token_seq = tokenizer(excerpt, padding = \"max_length\", max_length = MAX_LENGTH, truncation = True, return_tensors = \"pt\")\r\n","    token_seq.to(device)\r\n","    pred = model(**token_seq) # Unpack token sequences tensor \r\n","    preds_targets.append(pred.logits[0].item())\r\n","    \r\n","preds_targets"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["[-0.007161587942391634,\n"," -0.3461619019508362,\n"," -0.18076074123382568,\n"," -2.6938998699188232,\n"," -1.7281757593154907,\n"," -0.8599075675010681,\n"," 0.5470359325408936]"]},"metadata":{},"execution_count":22}],"metadata":{"execution":{"iopub.status.busy":"2021-09-17T04:33:14.894445Z","iopub.execute_input":"2021-09-17T04:33:14.894718Z","iopub.status.idle":"2021-09-17T04:33:15.051137Z","shell.execute_reply.started":"2021-09-17T04:33:14.894688Z","shell.execute_reply":"2021-09-17T04:33:15.050323Z"},"trusted":true}},{"cell_type":"code","execution_count":24,"source":["submission_df = pd.DataFrame({'id' : preds_data['id'], 'target': preds_targets})\r\n","submission_df"],"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>c0f722661</td>\n","      <td>-0.007162</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>f0953f0a5</td>\n","      <td>-0.346162</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0df072751</td>\n","      <td>-0.180761</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>04caf4e0c</td>\n","      <td>-2.693900</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0e63f8bea</td>\n","      <td>-1.728176</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>12537fe78</td>\n","      <td>-0.859908</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>965e592c0</td>\n","      <td>0.547036</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          id    target\n","0  c0f722661 -0.007162\n","1  f0953f0a5 -0.346162\n","2  0df072751 -0.180761\n","3  04caf4e0c -2.693900\n","4  0e63f8bea -1.728176\n","5  12537fe78 -0.859908\n","6  965e592c0  0.547036"]},"metadata":{},"execution_count":24}],"metadata":{"execution":{"iopub.status.busy":"2021-09-17T04:33:30.889825Z","iopub.execute_input":"2021-09-17T04:33:30.890388Z","iopub.status.idle":"2021-09-17T04:33:30.915551Z","shell.execute_reply.started":"2021-09-17T04:33:30.890352Z","shell.execute_reply":"2021-09-17T04:33:30.914670Z"},"trusted":true}},{"cell_type":"code","execution_count":25,"source":["submission_df.to_csv(\"submission.csv\", index = False)"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-09-17T04:34:10.295037Z","iopub.execute_input":"2021-09-17T04:34:10.295594Z","iopub.status.idle":"2021-09-17T04:34:10.304824Z","shell.execute_reply.started":"2021-09-17T04:34:10.295553Z","shell.execute_reply":"2021-09-17T04:34:10.304031Z"},"trusted":true}}]}